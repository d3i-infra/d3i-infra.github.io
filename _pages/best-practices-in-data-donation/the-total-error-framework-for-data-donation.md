---
layout: single-toc-on-top
classes: wide
title:  The total error framework for data donation
permalink: /best-practices-in-data-donation/the-total-error-framework-for-data-donation
toc: true
sidebar:
  nav: "best-practices-in-data-donation"
---

# The total error framework

The Total Error framework (TE framework) for data donation by Boeschoten, Araujo, et al. (2022) summarizes potential sources of error that can influence data collection. The TE framework is depicted in Figure 9. In this section, this TE framework will be explained in more detail. Then it will be discussed how our proposed workflow aligns with the TE framework. It will be illustrated that all error sources considered by the TE framework are taken into account with the proposed workflow, as illustrated in figure 10. This forms support for the steps as described in proposed workflow.

The TE framework consists of a measurement side and a representation side. The measurement side comprises error sources that are introduced by the decisions on how the constructs in the study are being measured, and consists of construct invalidity, measurement error, extraction error, algorithmic error, and integration error respectively. The representation side comprises error sources related to the target population that the data is meant to represent. It consists of coverage error, sampling error, nonresponse error, compliance error, and consent error respectively.


# Defining the study purpose of the TE framework 

Concretizing the data donation research idea (the first step of the workflow; section 3) aims to account for construct invalidity and measurement error. Construct invalidity occurs when the collected data does not align well with the construct aimed to measure. In that case, the collected data does not measure the construct, resulting in made inferences being invalid. An example where construct invalidity potentially can be found, is in the RQ2 (WhatsApp; see section 2.3). The construct ‘responding most to’ was measured as the number of messages sent directly after another person. It can be argued that this operationalization does not take into account responses to a message that are sent with some messages in between. Therefore, construct invalidity might play a role here to a certain extend. The steps described in section 3 aim to guide the researcher in avoiding construct invalidity.

Measurement error occurs when incorrect or imprecise measurements are made. Such incorrect measurements can be biased and might lead to incorrect inferences. Incorrect measurements should be avoided, or be accounted for if they cannot be prevented. As measurements in data donation studies are mostly carried out by the platforms of interest, ensuring correct measurement is not always feasible. However, mapping allegedly incorrect measurements and accounting for these measurements is very important. In the workflow, measurement error is considered in the steps in section 3 by comprehending the DDPs of interest, and by mapping and accounting for potential limitations of platforms and DDPs of interest.

**Figure 9**

![Figure 9: Total error framework for social-scientific data collection with DDPs as in Boeschoten, Araujo, et al. (2022). All steps in data collection in data donation studies and possible resulting errors are shown.](/assets/images/about/data_donation_TEF.png)

Total error framework for social-scientific data collection with DDPs as in Boeschoten, Araujo, et al. (2022). All steps in data collection in data donation studies and possible resulting errors are shown.

# Constructing the study design and the TE framework

Construction of the data donation study design (section 4) considers all five error sources from the representation side of the TE framework. Coverage error and sampling error both are relevant for any study where inferences for a population are made. Coverage error occurs when the researcher fails to include people from the target population to potentially participate in the study. If there is a systematic bias between the people not included, bias might emerge in the data. In data donation studies, coverage error can specifically emerge when the population of interest is not the same population as users of the platform of interest. For example, if for example RQ 1 (GSLH) the population of interest is the general population, this does not align perfectly with the population of people using GSLH and owning a smartphone. People without a smartphone or Google account might travel differently than people with a smartphone and Google account, resulting in potential bias in population inferences made. The recommendations in the first step of the workflow (section 3) aim to take into account a discrepancy between the population of interest and study population, and subsequently to account for coverage error. On the other hand, sampling error emerges when the researcher is unsuccessful in obtaining a representative study sample. In order to avoid both coverage error and sampling error, the proposed workflow recommends to seek advise from a methodological expert when constructing the study design. 

The other three representation errors can be linked to the participant flow. First, non-response error can arise when people decide not to participate in the study. Another term describing non-response commonly used is willingness to participate. Depending on the exact study design, it can be more or less difficult to distinguish generic non-response from non-response caused by the choice include data donation in the study design and the perception participants might have about data donation. As recommended, clear communication and insight in the privacy risks, and incentives can be used to achieve higher response rates and minimize non-response error. Furthermore, additional information on participants not participating in the donation can be used to account for non-response bias.

Second, compliance error arises when participants drop out anywhere in the participant flow as a result of being unable to follow the actions requested in the participant flow. This error could result in bias if groups being able and unable to comply differ. For example, when unclear DDP download instruction are provided, technologically proficient participants are more likely to comply than participants that do not use the internet regularly. The proposed workflow accounts for compliance error by stimulating for provision of detailed and correct instructions for each step of the participant flow, tailored towards the population and differentiated for different operational systems and devices. Other actions to account for compliance error could be including an assistance point for participants with questions and extensively testing the participant flow, for example by using cognitive interviews (Collins, 2014) to understand how participants experience the study.

Third, consent error emerges when participants decide to not donate in the last step of the participant flow. This could for example happen when the participant did not experience their privacy as guaranteed during the participant flow. Another aspect which might influence consent error, is the visualisation of the data extracted from the DDP. Visualisations might aid understanding of what information is exactly send to the researcher. If participants cannot oversee what information is exactly donated, they might tend to decline the donation, resulting in potential bias. In sections 4, 5, and 7, the privacy of participants and visualization of the data are addressed in the workflow, ensuring consent error is taken into account.

Including an additional data collection method in the study design (as described in section 4), can help in accounting for both measurement error and non-response error. By providing the opportunity to measure a construct in multiple ways (e.g. both through data donation and through a questionnaire), multiple measurements can be combined to a single measurement with multiple indicators (for example through multitrait multimethods models; Oberski et al., 2017). These measurements hold more information than measurements with a single indicator, and measurement error can be reduced by this. Furthermore, non-response error can be taken into account by collecting contextual information on participants unwilling to participate in data donation. By explaining differences between participants and non-participants through these data, missingness in data can transform from missing not at random to missing at random (van Buuren, 2018). Non-response error can then be accounted for by for example weighting (Boeschoten, Araujo, et al., 2022).

**Figure 10**

![Figure 10: Error sources of the TE framework and steps in the workflow taking into account each of these error sources.](/assets/images/about/TEF_workflow_alt2_v2.png)

Error sources of the TE framework and steps in the workflow taking into account each of these error sources.

# Feature extraction and the TE framework

In building the feature extraction software (section 5), the workflow aims to avoid extraction error and algorithmic error emerging in the data. Extraction error occurs when software fails to find and extract the correct data in DDPs. This can either result in data not being extracted or in the incorrect data being extracted. Both can lead to biased data and invalid results. The workflow addresses accounting for extraction error by accounting for variability of DDPs over devices, settings, and time. The workflow steps support a robust extraction script, combating extraction error.

Algorithmic error describes any error in the data as a result of using an algorithm within the data collection. Algorithms are seldom perfect, and therefore some algorithmic error will always be present in the data on use of algorithms. For example, in RQ 3 (WhatsApp), an algorithm could be used to analyze text and decide whether a text message was a response on an earlier message. Not every reply will then correctly be classified as such, potentially biasing the data and inferences based on it. Algorithmic error is not specific to data donation and occurs in any data collection that makes use of algorithms. Therefore, it is hard to account for this error. In the workflow, it is recommended to not use algorithms excessively, and study the properties and performances of algorithms before incorporating these in the study.

# Remainder of the workflow and the TE framework 

The ethical considerations and recommendations in the workflow (section 7) are mostly prerequisites for any study concerning human participants. However, the points raised here in the workflow aim to account for non-response error and consent error as well. Ensuring good privacy and good communication towards participants about the privacy will help in convincing participants of the message that the study procedure holds no privacy risks for participants. If this message is not conveyed clear enough, participants might drop out from the study and collected data can be biased as a result of this.

The recommendations in section 8 aim to account for integration error and compliance error. Integration error emerges when data from multiple sources about a single participant is not matched correctly. In such case, data on a single participant can be interpreted as data about multiple participants, and inferences can be biased as a result of this. As data linkage is explicitly considered and recommended when conducting a pilot study, mistakes in data linkage can be identified and taken into account, minimizing potential integration error. Furthermore, conducting a pilot study helps in identifying difficulties and errors in the participant flow. These findings can be taken into account, ensuring participants will be able to complete the participant flow. Therefore, these improvements help in minimizing the effect of compliance error on the data quality.
